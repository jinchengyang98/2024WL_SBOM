# Tracer: 自动化OSS漏洞补丁定位系统

## 目录
1. [系统概述](#系统概述)
2. [系统架构](#系统架构)
3. [数据搜集](#数据搜集)
   - 3.1 [数据源](#数据源)
   - 3.2 [数据收集方法](#数据收集方法)
   - 3.3 [数据存储](#数据存储)
4. [补丁定位](#补丁定位)
   - 4.1 [URL分析](#url分析)
   - 4.2 [文本分析](#文本分析)
   - 4.3 [版本控制系统集成](#版本控制系统集成)
5. [评估](#评估)
   - 5.1 [准确性评估](#准确性评估)
   - 5.2 [覆盖率评估](#覆盖率评估)
   - 5.3 [性能评估](#性能评估)
6. [系统运行逻辑](#系统运行逻辑)
   - 6.1 [参考网络构建](#参考网络构建)
   - 6.2 [补丁选择](#补丁选择)
   - 6.3 [补丁扩展](#补丁扩展)
7. [配置和使用](#配置和使用)
8. [未来工作](#未来工作)
9. [术语表](#术语表)

## 系统概述

Tracer是一个自动化系统，旨在为开源软件(OSS)漏洞定位补丁。系统通过分析多个咨询来源中的漏洞报告、讨论和解决方案，识别和定位与特定CVE（通用漏洞和暴露）相关的补丁。

## 系统架构

[在这里插入一个简单的系统架构图展示主要组件及其关系]

## 数据搜集

1. 数据源

   - NVD (国家漏洞数据库)
     - 文件: utility/NVD/CVE_metadata_util.py
     - 主要函数: 
       - query_dataset(): 根据CVEID从NVD数据集中查询漏洞信息
         - 函数首先检查本地缓存，如果存在则直接返回
         - 如果本地没有，则构造API请求URL，发送GET请求到NVD API
         - 解析返回的JSON数据，提取相关字段
       - extract_NVD_metadata(): 从NVD原始数据中提取关键元数据
         - 提取的信息包括：CVEID, 描述, 参考链接, CVSS评分, CWE, 影响的产品等
         - 对于参考链接，进行初步分类（如补丁、问题跟踪、第三方公告等）

   - Debian 安全跟踪器
     - 文件: utility/debian_util.py
     - 主要函数:
       - get_refs_in_Debian_report(): 获取Debian报告中的参考链接
         - 函数首先检查本地缓存的Debian安全跟踪数据
         - 如果数据不存在或过期，则更新本地数据（可能涉及爬取Debian安全跟踪器网站）
         - 从更新后的数据中提取与特定CVE相关的所有引用

   - Red Hat Bugzilla
     - 文件: utility/crawler/crawl_util.py
     - 主要函数:
       - request_url(): 发送HTTP请求获取Red Hat Bugzilla页面内容
         - 函数处理可能的网络错误、重定向和超时情况
         - 实现请求重试机制，以应对临时网络问题
         - 遵守网站的robots.txt规则，实现请求间隔以避免过度频繁的访问

   - GitHub 仓库
     - 文件: utility/crawler/github_util.py
     - 主要函数:
       - search_github_code(): 使用GitHub API搜索代码仓库
         - 构造搜索查询，包含CVE ID和可能的关键词（如"vulnerability", "fix", "patch"等）
         - 处理GitHub API的速率限制，实现令牌轮换和请求节流
         - 解析API返回的结果，提取相关的仓库信息、文件路径和代码片段

2. 数据收集方法

   - 使用爬虫从各数据源获取漏洞信息
     - 文件: utility/crawler/crawl_util.py
     - 主要函数:
       - request_and_judge_response(): 发送请求并判断响应的有效性

   - 解析HTML内容提取相关数据
     - 文件: utility/anlalyse_webcontent_util.py
     - 主要函数:
       - extract_URLs_and_text(): 从HTML提取URL和文本内容
         - 函数需要处理各种HTML结构，包括普通链接、JavaScript生成的链接等
         - 实现对动态加载内容的处理，可能需要使用Selenium等工具

   - 使用API获取结构化数据(如GitHub API)
     - 文件: utility/crawler/github_util.py
     - 主要函数:
       - github_graphql_request(): 发送GraphQL请求到GitHub API

3. 数据存储

   - 使用JSON格式存储原始和处理后的数据
     - 文件: utility/json_processing.py
     - 主要函数:
       - write(): 将JSON内容写入文件
       - read(): 从文件读取JSON内容

   - 使用文件系统组织不同类型的数据
     - 文件: utility/file_processing.py
     - 主要函数:
       - creatFolder_IfExistPass(): 创建文件夹，如果已存在则跳过
       - walk_FileDir(): 遍历目录获取所有文件路径

## 补丁定位

1. URL分析

   - 从漏洞描述和参考链接中提取URL
     - 文件: utility/anlalyse_webcontent_util.py
     - 主要函数: 
       - extract_URLs_issueKey_commitID_from_text(): 从文本中提取URL、issue key和commit ID
         - 函数使用正则表达式提取文本中的URL、issue key和commit ID
         - 对于URL，进行规范化处理（如处理相对路径、去除无关参数等）

   - 识别包含补丁信息的URL(如diff、commit等)
     - 文件: utility/anlalyse_webcontent_util.py
     - 主要函数:
       - identify_diff_text(): 判断文本是否包含diff信息

2. 文本分析

   - 使用正则表达式识别补丁相关的文本模式
     - 文件: utility/anlalyse_webcontent_util.py
     - 主要函数:
       - extract_URLs_issueKey_commitID_from_text(): 使用正则表达式提取关键信息

3. 版本控制系统集成

   - 支持Git仓库的补丁定位
     - 文件: utility/crawler/github_util.py
     - 主要函数:
       - get_commit_content(): 获取特定commit的内容

   - 通过commit ID或branch名称定位具体补丁
     - 文件: utility/crawler/github_util.py
     - 主要函数:
       - get_branch_commit(): 获取特定分支的最新commit

## 评估

1. 准确性评估

   - 计算补丁定位的精确率、召回率、F1分数等指标
     - 文件: utility/evaluation.py
     - 主要函数:
       - calculate_precision_recall_accuracy_error_f1(): 计算各项评估指标

   - 与人工标注的ground truth数据集进行对比
     - 文件: utility/evaluation.py
     - 主要函数:
       - calculate_from_tp_fp_fn_tn(): 根据真假正负例计算评估指标

2. 覆盖率评估

   - 统计能够成功定位补丁的CVE比例
     - 文件: patch_localization_tool_manager.py
     - 主要函数:
       - evaluate_patch_localization(): 评估补丁定位的覆盖率

   - 分析不同数据源和补丁类型的覆盖情况
     - 文件: patch_localization_tool_manager.py
     - 主要函数:
       - analyze_coverage_by_source(): 分析不同来源的覆盖情况

3. 性能评估

   - 记录补丁定位的耗时
     - 文件: utility/common_util.py
     - 主要函数:
       - current_timestamp(): 获取当前时间戳

   - 分析系统的可扩展性
     - 文件: patch_localization_tool_manager.py
     - 主要函数:
       - analyze_scalability(): 分析系统在不同规模数据下的性能

## 系统运行逻辑

Tracer系统的运行逻辑分为三个主要步骤，每个步骤都涉及多个子任务和复杂的处理逻辑：

1. 参考网络构建

   a. 咨询分析：从多个咨询源收集CVE信息
      - NVD数据处理：
        - 使用 `utility/NVD/CVE_metadata_util.py` 中的 `query_dataset()` 函数根据CVE ID查询NVD数据集
          - 函数首先检查本地缓存，如果存在则直接返回
          - 如果本地没有，则构造API请求URL，发送GET请求到NVD API
          - 解析返回的JSON数据，提取相关字段
        - 使用 `extract_NVD_metadata()` 函数从原始NVD数据中提取关键信息
          - 提取的信息包括：CVEID, 描述, 参考链接, CVSS评分, CWE, 影响的产品等
      - Debian报告处理：
        - 使用 `utility/debian_util.py` 中的 `get_refs_in_Debian_report()` 函数获取Debian安全跟踪器中的CVE相关报告
          - 函数使用 `json_processing.read(config.DEBIAN_CVE_REPORT_LIST)` 读取Debian CVE报告列表
          - 从读取的数据中提取与特定CVE相关的所有引用
      - Red Hat Bugzilla数据获取：
        - 使用 `utility/crawler/crawl_util.py` 中的 `request_url()` 函数发送HTTP请求获取Red Hat Bugzilla页面内容
          - 函数处理可能的网络错误、重定向和超时情况
      - GitHub仓库搜索：
        - 使用 `utility/crawler/github_util.py` 中的 `search_github_code()` 函数通过GitHub API搜索与CVE相关的代码仓库
          - 构造搜索查询，包含CVE ID和可能的关键词（如"vulnerability", "fix", "patch"等）

   b. 参考分析：分析收集到的参考，将节点分类
      - 使用 `utility/anlalyse_webcontent_util.py` 中的 `extract_URLs_issueKey_commitID_from_text()` 函数处理每个参考链接
        - 函数使用正则表达式提取文本中的URL、issue key和commit ID
      - 根据URL特征和内容将节点分类为：
        - 补丁节点：URL包含git或svn相关字符串
        - 问题节点：URL包含常见问题跟踪系统的关键词
        - 混合节点：不符合上述两类的其他节点

   c. 参考增强：进一步分析问题和混合节点，提取更多相关引用
      - 对于每个问题或混合节点：
        - 使用 `utility/anlalyse_webcontent_util.py` 中的 `extract_URLs_and_text()` 函数提取页面中的所有URL
        - 对提取的URL重复进行参考分析过程

2. 补丁选择

   a. 置信度评估：选择高置信度的补丁节点
      - 在 `patch_localization_tool_manager.py` 中实现选择逻辑
        - 直接来自NVD咨询源的补丁节点
        - 直接来自GitHub咨询源且commit message包含CVE标识符的补丁节点

   b. 连接性分析：计算补丁节点与根节点(CVE)的连接性
      - 在 `patch_localization_tool_manager.py` 中实现连接性计算
        - 结合路径数量和长度，计算一个综合的连接性得分

3. 补丁扩展

   对于每个选中的GitHub commit类型的补丁：
   a. 收集仓库所有分支信息
      - 使用 `utility/crawler/github_util.py` 中的函数与GitHub API交互
      - 获取仓库的所有分支列表

   b. 在每个分支的特定时间跨度内搜索相关commit
      - 使用 `utility/crawler/github_util.py` 中的 `get_commit_content()` 函数获取每个commit的详细内容

   c. 判断commit是否为相关补丁
      - 使用正则表达式分析commit message，查找与原始补丁commit message相同或包含关系

   d. 将符合条件的commit作为扩展补丁添加到结果中
      - 使用 `utility/json_processing.py` 中的函数将扩展的补丁信息存储到JSON文件中

在整个过程中，系统会频繁使用 `config.py` 中定义的配置项，如数据路径、API密钥、日志设置等。同时，会使用 `utility/evaluation.py` 中的函数持续评估补丁定位的准确性和覆盖率，以便进行实时调整和优化。

## 配置和使用

- 配置文件: config.py
  - DATA_ROOT_PATH: 数据根目录，用于存储所有数据文件
  - OUTPUT_PATH: 输出目录，用于存储系统生成的结果
  - LOG_LEVEL: 日志级别，可选值包括DEBUG, INFO, WARNING, ERROR, CRITICAL
  - NVD_DATASET_ITEMS_DIR: NVD数据集目录，存储从NVD下载的原始数据
  - [添加其他重要配置项的说明]

- 主要入口: patch_localization_tool_manager.py
  - 主要函数:
    - main(): 程序入口点
    - run_patch_localization(cve_list): 运行补丁定位流程

- 日志: 使用Python logging模块,输出到指定文件
  - 配置位置: config.py
  - 主要日志文件:
    - LOG_PATCH_LOCALIZATION_TOOL_MANAGER: 补丁定位工具管理器日志
    - LOG_CRAWLING_GITHUB_ALL_INFORMATION_PATH: GitHub爬取信息日志

## 未来工作

1. 支持更多的漏洞数据源
2. 改进补丁相关性判断算法
3. 实现机器学习模型以提高补丁识别准确率
4. 支持实时漏洞补丁追踪

## 术语表

- CVE: Common Vulnerabilities and Exposures，通用漏洞和暴露
- NVD: National Vulnerability Database，国家漏洞数据库
- OSS: Open Source Software，开源软件
- API: Application Programming Interface，应用程序编程接口